\chapter{Related Work}\label{ch:background-literature}
There is a considerable amount of research in the area of pricing and resource allocation in cloud computing,
of which some use auction mechanisms to deal with competition~\citep{KUMAR2017234,Zhang2017,Du2019,Bi2019} as this project does.
Therefore section~\ref{sec:related-work-in-cloud-computing} presents the related approaches to resource allocation
in cloud computing and edge cloud computing to the one taken in this project.

The proposed solution of the project (presented in chapter~\ref{ch:proposed-solution-to-problem}) uses a form of
machine learning, called reinforcement learning. Section~\ref{sec:related-work-in-machine-learning} explores the
research and the current state of the art algorithms of deep Q learning and policy gradient that are used in this
project.

\section{Related Work in Cloud Computing}\label{sec:related-work-in-cloud-computing}
A majority of approaches for pricing and resource allocation in cloud computing use a fixed resource allocation
mechanism such that user request a fixed amount of certain resource from the cloud provider. However this mechanism
provides no control over the resources quantity allocated, only to which server these resources are allocated to.
Therefore a majority of this resources is focusing on designing efficient and incentive compatible auction mechanism.
A survey of these approaches has been investigated by~\cite{} and~\cite{} that found ....
%% Todo on pricing on cloud computing

%A majority of the approaches for pricing and resource allocation in cloud computing require users to request a
%fixed amount of certain resource with the cloud provider having no control over the resources only the servers that the
%task was allocated to~\citep{KUMAR2017234,Zhang2017,Du2019,Bi2019}.

Other closely related work on resource allocation in edge clouds~\cite{vaji_infocom} considers both the placement of
code/data needed to run a specific task, as well as the scheduling of tasks to different edge clouds. The goal there
is to maximize the expected rate of successfully accomplished tasks over time. Our work is different both in the setup
and the objective function. Our objective is to maximize the value over all tasks. In terms of the setup, they assume
that data/code can be shared and they do not consider the elasticity of resources.

Previous work by this author in~\cite{FlexibleResourceAllocation} proposed the novel resource allocation (explained in
chapter~\ref{ch:project-problem}) along with an optimisation problem mathematically describing the resource allocation.
This work then continued and presents three solutions for the problem case, a greedy algorithm to quickly approximate
a solution in order to maximise the social welfare and two auction mechanisms as server are normally paid for usage
of their resources. The greedy algorithm is a polynomial time algorithm that will find solution within $\frac{1}{n}$
of the optimal social welfare. This is done through the use of modular heuristics for ordering the task by density
then for each task, select a server based on available resource on each servers then to allocate resources that
minimises a resource heuristics. Using certain heuristics, the greedy algorithm achieves at least 90\% of the optimal
solution and 20\% more than optimal solution for fixed resource equivalent problems. The first of the auction mechanisms
is a novel distributed iterative auction developed using a reverse vcg principle to calculate a task price. That meant
that a task doesnt need to reveal its private value also that the auction could be run in a decentralised way.
This means that the auction is budget balanced however it is not economically efficient or incentive compatible.
The third algorithm is an implementation of a single parameter auctions~\citep{nisan2007algorithmic_critical_value}
using the greedy algorithm to find the critical value of a task. Using this mechanism with a monotonic value density
heuristic results in the auction being incentive compatible.

\section{Related Work in Reinforcement learning}\label{sec:related-work-in-machine-learning}
Computer scientists have always been interested in testing computers against humans (cite turing test) and a key part
is the ability to learn. For computers this ability is much more complex and researchers have
found a variety of ways to allow computers to do this. These methods are broadly grouped into three categories of
supervised, unsupervised and reinforcement learning. Supervised learning uses pairs of inputs to true outputs like in
case of image classifications where each image has a correct category for the image to be mapped to. Unsupervised
learning instead doesn't have a true output meaning that algorithms tries to find links between similar data.

However both of these methods are not effective for real world interactions as agents must make a series of
actions that result in rewards. Algorithms designed for these problems with a series of actions fall into the category
of reinforcement learning which aims to maximise the reward over time. This is the area of machine learning that this
project utilises as the problem can be modelled as a markov decision process (in section~\ref{}) that allows agents
to interact with the environment to learn over time. Reinforcement learning is a rapidly growing field of research
within AI due to its real world applications like driving a car, playing games, etc.

Q-learning algorithm~\cite{} is a learning method used for estimating the action-value function that is one of the
bases on for modern reinforcement learning. As the series of actions can be formed into a tree of actions, an agent
is interested in which actions will result in the largest reward in the future. This is formulated as in
equation~\eqref{eq:q_learning}.
\begin{align}
    Q(s_t, a_t) = Q(s_t, a_t) + \alpha \cdot (r_t + \lambda \cdot \text{max}_a Q(s_{t+1} , a) - Q(s_t, a_t) ) \label{eq:q_learning}
\end{align}

However the curse of dimensionally was found to be a major problem as to use Q learning required
forming a table of state-actions in order to calculate and as the number of dimensions increase, the number of
state-actions will increase exponentially. Therefore making the method impractical for problem that had large state
space. Therefore use of a function approximate is used to circumvent this problem, traditionally done using a neural
network. Work by~\cite{} using a deep convolution neural network was able to achieve state of the art in six of seven
games tried on atari with three of these scores being superhuman. This work was followed up by~\cite{} and found that
with no modifications to the hyperparameters, neural network and training method; state of the art results were
achieved in ... of the ... with superhuman in ... of these. Additional heuristics have been proposed deep Q learning:
double DQN~\cite{}, prioritized experience replay~\cite{}, dueling network architecture~\cite{}, multi-step bootstrap
targets~\cite{}, A3C~\cite{}, distributional Q-learning~\cite{} and noisy DQN~\cite{}. These methods were combined
to together~\cite{}, called rainbow DQN, achieving over 200\% of the original DQN algorithm and over 50\% than any optimisation on its
own in a quarter of the observations.

Using the base of Q-learning, policy gradient separate the action selection policy to the q-value policy. In Q-learning,
the selection the action is based on the maximum Q-value of the actions however policy gradient separates. This has
the advantages of being able to deal with both discrete and continuous action spaces where Q-learning can only deal
with discrete action space. Also the learning method doesn't require \epsilon-greedy action selection that can for
Q-learning cause the resulting policy to differ from the optimal policy. Therefore policy gradient has been used to
master the game of Go~\citep{silver2017mastering} and achieve top 1\% in Dota 2~\citep{OpenAI_dota} and
Starcraft 2~\citep{starcraft2}.
