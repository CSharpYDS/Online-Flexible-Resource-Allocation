\chapter{Literature Review}\label{ch:literature-review}
There is a considerable amount of research in the area of resource allocation and task pricing in cloud computing,
in case auction mechanisms are used to deal with competition~\citep{KUMAR2017234,Zhang2017,Du2019,Bi2019}.
Section~\ref{sec:resource-allocation-and-auctions-in-cloud-computing} presents the different approaches to resource
allocation and pricing mechanisms in cloud computing.

The proposed solution of the project (presented in chapter~\ref{ch:proposed-solution}) uses a form of
machine learning, called Reinforcement Learning. Section~\ref{sec:reinforcement-learning} covers the current
state-of-the-art algorithms with Q learning and policy gradient research.

\section{Resource allocation and auctions in Cloud Computing}\label{sec:resource-allocation-and-auctions-in-cloud-computing}
A majority of approaches taken for task pricing and resource allocation in cloud computing uses a fixed resource
allocation mechanism, such that each user requests a fixed amount of resources from a server for a task. However this
mechanism, as previously explained, provides no control for the server over the quantity of resource allocated to task,
only determining the task price. As a result, a majority of approaches don't consider the server management of resource
allocation. Thus research has focused on designing efficient and strategyproof auction mechanisms.

Work by~\cite{KUMAR2017234} provides a systematic study of double auction mechanisms for cloud computing covering
inter-cloud ("Cloud of Clouds") auctions~\citep{interclouds}. Inter-cloud enable auction with multiple cloud service
provider which aims to provide advantages to both cloud provider and customer such as improved quality of services for
customer, increased cost efficiency, better services during peak times and reduced overall energy consumption.
The work reviewed 21 different proposed auction mechanism over a range of important properties like Economic Efficiency,
Incentive Compatibility and Budget-Balance. In a majority of the proposed auction mechanisms, truthfulness was only
considered for the user, thus a truthful multi-unit double auction mechanism was presented as novel double-sided
truthful auction meaning both users and server should act truthfully.

Deep reinforcement learning was implemented by~\cite{Du2019} to learn resource allocation and pricing in order to
maximise cloud profits. Deep neural network models with long/short term memory units enabled state-of-the-art online
cloud resource allocation and task pricing algorithms that had significantly better results than traditionally
online mechanisms in terms of profit made and number of users accepted. The system considered both the pricing and
placement of virtual machines in the system to maximise the profits of cloud providers through the use of deep
deterministic policy gradient~\citep{ddpg} to train agents. Users would request a type of virtual machine from the
system that a server would allocate to user where the price and placement of the virtual machine within possible
servers by a neural network agent. The deep reinforcement learning models were trained using real-world cloud workloads
and achieved significantly high profit even in worst-case scenarios.

Some approaches have been taken to increase flexible within fog cloud computing by~\cite{Bi2019} enabled efficient
distribution of data centers and connections to maximise social welfare. A truthful online mechanism was
proposed that was incentive compatible and individually rational that allows tasks to arrive over time by solving a
integer programming optimisation problem. Similar research in~\cite{vaji_infocom}, considers the placement of code/data
needed to run specific tasks over time as demand changes while considering operational costs and system stability.
An approximation algorithm achieved 90\% of the optimal social welfare by converting the problem to a set function
optimisation problem.

Previous work by this author in~\cite{FlexibleResourceAllocation} proposed the novel resource allocation mechanism and
optimisation problem that this project works to expand. The paper presents three mechanism for the optimisation problem,
one to maximise the social welfare and two auction mechanisms for self-interested users. The Greedy algorithm presented
allows for quick approximation of a solution through the use of several heuristics in order to maximise the social
welfare. Results found that the algorithm achieved over 90\% of the optimal solution given certain heuristics compared
to fixed resource allocation solution that achieved 70\%. The algorithm has polynomial time complexity with a lower
bound of $\frac{1}{n}$ however in practice achieves significantly better results. \\
The work also presented a novel decentralised iterative auction mechanism developed using the VCG
mechanism~\citep{vickrey, Clarke, groves} in order to iterative increase a task's price. As a result, a task doesn't
reveal its private task value. This may be particularly interesting with military tactical network such that countries
do not need to reveal the important of a task to another coalition country. The auction mechanism achieves over 90\% of
the optimal solution due to iteratively solving a specialised server optimisation problem. The third algorithm is an
implementation of a single parameter auction~\citep{nisan2007algorithmic_critical_value} using the greedy algorithm to
find the critical value for each task. Using the greedy algorithm with a monotonic value density function means the
auction is incentive compatible and inherits the social welfare performance and polynomial time complexity of the
greedy mechanism.

\section{Reinforcement learning}\label{sec:reinforcement-learning}
Computer scientists have always been interested in comparing computers against humans~\citep{turing1950computing} and a
key characters of humans is the ability to learn from experience. For computers this ability is complex to perform with
researchers finding a variety of ways for computers to do this. These methods are broadly grouped into three categories:
supervised, unsupervised and reinforcement learning. Supervised learning uses inputs that mapped to outputs, an example
is image classifications. While unsupervised learning doesn't have a known output for inputs to be mapped to, instead
these algorithms tries to find links between similar data, for example data clustering.

However both of these techniques are not applicable for case where agents must interact with an environment making a
series of actions that result in rewards over time. Algorithms designed for these problems fall into the category of
Reinforcement Learning that aim to maximise the agent rewards. These algorithms utilise environments that can be
formulated as a Markov Decision Process~\citep{Bel}. Because of the number of application for reinforcement learning,
it is a rapidly growing field of research within AI particularly in the last 5 years.

% Figure reinforcement learning model

\begin{figure}[h]
    \includegraphics[width=10cm]{figures/reinforcement_learning.png}
    \caption{Reinforcement learning model (Source: ~\cite{Sutton1998})}
    \label{fig:reinforcement_learning}
\end{figure}


Q-learning algorithm~\cite{watkins1992q-learning} is a learning method used for estimating the action-value function,
one of the bases on for modern reinforcement learning. As the series of actions can be formed into a tree of
actions, an agent is interested in which actions will result in the largest reward in the future. This is formulated
as in equation~\eqref{eq:q_learning}.

\begin{align}
    Q(s_t, a_t) = E[R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+2} + \cdots ] \label{eq:q_value} \\
    Q(s_t, a_t) = Q(s_t, a_t) + \alpha \cdot (r_t + \gamma \cdot \text{max}_a Q(s_{t+1} , a) - Q(s_t, a_t) ) \label{eq:q_learning} \\
\end{align}
% TODO
However the curse of dimensionally was found to be a major problem as to use Q learning required
forming a table of state-actions in order to calculate and as the number of dimensions increase, the number of
state-actions will increase exponentially. Therefore making the method impractical for problem that had large state
space. Therefore use of a function approximate is used to circumvent this problem, traditionally done using a neural
network. Work by~\cite{atari} using a deep convolution neural network was able to achieve state of the art in six of seven
games tried on atari with three of these scores being superhuman. This work was followed up by~\cite{mnih2015humanlevel}
and found that with no modifications to the hyperparameters, neural network and training method; state of the art results were
achieved in almost all 49 atari games and superhuman results in 29 of these games. Additional heuristics have been
proposed for deep Q learning: double DQN~\citep{doubledqn}, prioritized experience replay~\citep{prioritizedexperiencereplay},
dueling network architecture~\citep{duelingdqn}, multi-step bootstrap targets~\citep{multi-step-dqn, Sutton1998},
A3C~\cite{A3C}, distributional Q-learning~\citep{distributional_dqn} and noisy DQN~\citep{noisy_dqn}. These methods were
combined to together~\cite{rainbow}, called rainbow DQN, achieving over 200\% of the original DQN algorithm and over
50\% than any optimisation on its own in a quarter of the observations.

\begin{figure}[h]
    \includegraphics[width=10cm]{figures/actor-critic.png}
    \caption{Actor Critic model (Source: ~\cite{Sutton1998})}
    \label{fig:actor-critic-model}
\end{figure}

Using the base of Q-learning, policy gradient~\ref{fig:actor-critic-model} separate the action selection policy to the
q-value policy. In Q-learning, the selection the action is based on the maximum Q-value of the actions however policy
gradient separates. This has the advantages of being able to deal with both discrete and continuous action spaces where
Q-learning can only deal with discrete action space. Also the learning method doesn't require $\epsilon$-greedy action
selection that can for Q-learning cause the resulting policy to differ from the optimal policy. Therefore policy
gradient has been used to master the game of Go~\citep{silver2017mastering} and achieve top 1\% in
Dota 2~\citep{OpenAI_dota} and Starcraft 2~\citep{starcraft2}.