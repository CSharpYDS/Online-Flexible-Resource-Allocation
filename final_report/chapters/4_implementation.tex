\chapter{Implementation of the solution}\label{ch:implementation-of-the-solution}
Using the proposed solution in chapter~\ref{ch:proposed-solution-to-problem}, we can simulate edge cloud computing
environment and then train agents to bid on tasks and allocate resources. This has three sections to it, first the
simulation that is referred to as the environment (section~\ref{sec:simulating-the-environment}),
second to define the agents (section~\ref{sec:agent-definition}) and three to train the agents on the
environment (section~\ref{sec:training-of-agents}).

The implementation discussed below as written in Python and available to download on Github at
\url{https://github.com/stringtheorys/Online-Flexible-Resource-Allocation}

\section{Simulating the Environment}\label{sec:simulating-the-environment}
As the aim of the environment is to train agents to learn how to action optimally then the way that agents
interact with the environments matters. OpenAI~\cite{}, an independent research organisation, have create a large
number of open source popular reinforcement learning environment using the python module gym~\cite{}.
Due to the wide scale use and developing of agents to interact with environment using the OpenAI environment
specification, I have likewise programmed this environment to follow the same specification. %% todo this is a horrible sentence, to fix

Therefore each server is forced to have a individual agents for both of the subproblems making the overall problem
particularly interesting for multi-agent reinforcement learning environment. \\
This is because the overall aim of the environment is cooperative (to maximise the social welfare) but the servers
in competition during the auctions in order to maximise their profit.
But then the server must be cooperative in allocating of resources to each task as the server wish to complete as
many tasks as they can.

\section{Task pricing and resource weighting policy}\label{sec:agent-definition}
Using the problem definition from chapter~\ref{ch:project-problem}, the overall problem of flexible resource allocation
is be separated into two subproblems: the auctioning of tasks and the allocating of resources to tasks that each server
must do. Because of this, each server has to have a unique policy for bidding on tasks and allocating resources.
Even those the state space and action space are exactly the same, the optimal policies for task pricing and
resource allocation are completely different.

These policies are referred to as the task pricing agent, for bidding on tasks and the resource weighting agent,
for allocating of resources. However determining these policies is a difficult due to the complexity and the interaction
of task attributes and the server resources. Because of this, the ability of reinforcement learning to interact
an environment and learn a policy in order to maximise its reward over time. Therefore a range of different
policies have been implemented as Table~\ref{tab:policies} in order to evaluate the performance of different policies
learning methods.

\begin{table}
    \centering
    \begin{tabular}{l|l} \hline
        Policy Type & Explanation \\ \hline
        %% Todo add all of the policy comparisons
    \end{tabular}
    \caption{Table of policy types}
    \label{tab:policies}
\end{table}


\section{Training of agents}\label{sec:training-of-agents}
In order to train the agents certain hyperparameters were used.

\begin{table}
    \begin{tabular}{l|l} \hline
        Hyperparameter & Value \\ \hline
        %% todo add hyperparameter
    \end{tabular}
    \label{tab:dqn_hyperparameter}
    \caption{Table of Deep Q Networks training hyperparameters}
\end{table}

\begin{table}
    \begin{tabular}{l|l} \hline
        Hyperparameter & Value \\ \hline
        %% todo add hyperparameter
    \end{tabular}
    \label{tab:ddpg_hyperparameter}
    \caption{Table of Deep Deterministic Policy Gradient hyperparameters}
\end{table}
